\documentclass[final]{beamer}
\usepackage[orientation=portrait,size=a0,scale=1.25]{beamerposter}     
\geometry{hmargin=2.5cm,}
\usepackage[utf8]{inputenc}
\linespread{1.15}

% Custom theme
\usetheme{sharelatex}

% Packages
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}

% Title, authors, date
\title[Theoretical Principles of Deep Learning]{On Exact Computation with an Infinitely Wide Neural Net by Arora et al. (2019)}
\author{Review by Adonis Jamal}
\institute[CentraleSupélec]{CentraleSupélec}
\date{\today}


% ==============================================================================
% ================================== DOCUMENT ==================================  
% ==============================================================================
\begin{document}
\begin{frame}[t]
\begin{multicols}{2}


% ==============================================================================
% SECTION: CONTEXT AND MOTIVATION
% ==============================================================================
\section{Context and Motivation}
\textbf{The Question:} How does a Deep Neural Network behave when its width (number of channels/neurons) goes to infinity?

\begin{itemize}
    \item \textbf{Infinite Width Limit:} Connects Deep Learning to Gaussian Processes (GPs), allowing for exact analytical study.
    \item \textbf{Neural Tangent Kernel (NTK):} Describes the training dynamics of infinite networks under gradient descent \cite{jacot2020NTK}.
    \item \textbf{The Gap:} Previous works handled Fully Connected layers (FC). This paper provides the first exact, efficient algorithm for \textbf{Convolutional Neural Networks (CNTK)}.
\end{itemize}


% ==============================================================================
% SECTION: THEORETICAL GUARANTEE (PROOF SKETCH)
% ==============================================================================
\section{Theoretical Guarantee (Proof Sketch)}
\textbf{Theorem:} A fully-trained infinite-width net is equivalent to Kernel Regression using the CNTK.

\vspace{0.5em}
\textit{Proof Idea (Lazy Training Regime):}
The proof relies on analyzing the trajectory of weights $W(t)$ during gradient descent.

\begin{enumerate}
    \item \textbf{Dynamics:} The network output $f(x)$ evolves according to:
    $$ \frac{df(x)}{dt} = - \eta H(t) \cdot (f(x) - y) $$
    where $H(t)$ is the Neural Tangent Kernel.
    
    \item \textbf{Stability:} As width $m \to \infty$, the authors prove that weights stay infinitesimally close to initialization:
    $$ \|W(t) - W(0)\|_F \to 0 $$
    
    \item \textbf{Linearization:} Consequently, the kernel remains constant $H(t) \approx H(0)$. The complex non-linear training dynamics simplify to \textbf{linear regression} on the kernel features fixed at initialization.
\end{enumerate}


% ==============================================================================
% SECTION: METHODOLOGY (ALGORITHM)
% ==============================================================================
\section{The CNTK Algorithm}
Computing the kernel exactly requires propagating covariance matrices through the network layers.

\subsection{Recursive Convolution Step}
For patches of images $x$ and $x'$, the covariance $\Sigma^{(h)}$ at layer $h$ is computed from layer $h-1$: $$\Sigma^{(h)}(x, x') = c_\sigma \cdot \mathbb{E}_{(u, v) \sim \mathcal{N}(0, K^{(h-1)})} [\sigma(u)\sigma(v)]$$

\subsection{Efficient ReLU Implementation}
Instead of naïve sampling (slow), the paper uses a closed-form solution for the ReLU activation.

Let $\lambda$ be the correlation between two inputs: $$\lambda = \frac{\Sigma_{12}}{\sqrt{\Sigma_{11}\Sigma_{22}}}$$

The covariance after ReLU is exact: $$ V_{ReLU} = \frac{\sqrt{\Sigma_{11}\Sigma_{22}}}{2\pi} \left( \lambda (\pi - \arccos \lambda) + \sqrt{1 - \lambda^2} \right)$$

This allows for exact, vectorized computation on GPUs without Monte Carlo approximation.


% ==============================================================================
% SECTION: REPRODUCTION SETUP
% ==============================================================================
\section{Reproduction Setup}
\begin{itemize}
  \item \textbf{Code:} Implemented in PyTorch (Custom CNTK Module).
  \item \textbf{Data:} CIFAR-10 subset ($N=200$ images) due to $O(N^2)$ complexity.
\end{itemize}


% ==============================================================================
% SECTION: EXPERIMENTAL RESULTS
% ==============================================================================
\section{Experimental Results}

\textbf{The Depth Gap Phenomenon:} While Finite CNNs improve with depth, the CNTK performance saturates.

\begin{center}
  \includegraphics[width=0.95\linewidth]{../results/extension/depth_gap_analysis.png}
\end{center}


% ==============================================================================
% SECTION: CRITICAL ANALYSIS
% ==============================================================================
\begin{itemize}
  \item \textbf{Computational Cost:} The method scales as $O(N^2)$, making it intractable for full ImageNet without approximations.
  \item \textbf{Feature Learning:} The success of the "lazy" CTNK regime implies that feature learning is not strictly necessary for small data.
  \item \textbf{Limitations:} The performance gap on large data (Finite Nets > CNTK) suggests that feature learning (weights moving far from initialization) remains crucial for state-of-the-art performance.
\end{itemize}


% ==============================================================================
% SECTION: REFERENCES
% ==============================================================================
\section{References}
\bibliographystyle{plain}
\bibliography{bibliography}


\end{multicols}
\end{frame}
\end{document}
