\documentclass[11pt]{article}
\usepackage{theme}
\usepackage{shortcuts}

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage[numbers]{natbib}
\usepackage{float}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% Document parameters
% Document title
\title{Theoretical Principles of Deep Learning
      \\ \Large{CentraleSup√©lec MDS 2025/2026 - Paper Reading Assignment}
      \\ \Large{On Exact Computation with an Infinitely Wide Neural Net}}

\author{
Adonis JAMAL \email{adonis.jamal@student-cs.fr}
}

\date{February 1st, 2026}



% ==============================================================================
% ================================== DOCUMENT ==================================  
% ==============================================================================
\begin{document}
\maketitle

% ==============================================================================
% SECTION : INTRODUCTION AND CONTRIBUTIONS
% ==============================================================================
\section{Introduction and Contributions}
The theoretical study of deep learning often relies on the limit of infinite width to provide tractable analytical tools for understanding optimization and generalization \cite{main}. While the correspondence between infinite fully-connected networks and Gaussian Processes is well-established, the Neural Tangent Kernel (NTK) introduced by Jacot et al. \cite{jacot2020NTK} characterized the evolution of fully-trained networks in this regime. The paper \textit{On Exact Computation with an Infinitely Wide Neural Net} extends this framework to Convolutional Neural Networks (CNNs), addressing the computational infeasibility of evaluating kernels for modern architectures with pooling layers. The authors pose a central question: can the "infinite limit" of classic architectures like VGG or AlexNet achieve competitive classification performance on standard datasets such as CIFAR-10? To answer this, they derive the Convolutional NTK (CNTK) and provide the first efficient, exact algorithm for its computation, proving that a fully-trained, sufficiently wide network is equivalent to the kernel regression predictor using CNTK.

In this report, we focus on the paper's algorithmic contribution: the exact and efficient computation of the CNTK. We first detail the theoretical recursive formulas that allow for the exact evaluation of convolutional kernels with and without Global Average Pooling (GAP). We then describe our reproduction of the paper's main experimental result, implementing the exact dynamic programming algorithm to evaluate CNTK performance on the CIFAR-10 dataset. Our experiments confirm the significant performance gap between vanilla CNTK and CNTK-GAP architectures. Finally, we experimentally illustrate the authors' findings regarding network depth, verifying the distinct performance characteristics of infinite-width architectures compared to their finite counterparts across varying depths.


% ==============================================================================
% SECTION : PAPER OVERVIEW
% ==============================================================================
\section{Paper Overview}



% ==============================================================================
% SECTION : METHODOLOGY
% ==============================================================================
\section{Methodology}



% ==============================================================================
% SECTION : DATA AND EXPERIMENTAL SETUP
% ==============================================================================
\section{Data and Experimental Setup}



% ==============================================================================
% SECTION : RESULTS
% ==============================================================================
\section{Results}



% ==============================================================================
% SECTION : CONCLUSION AND DISCUSSION
% ==============================================================================
\section{Conclusion and Discussion}



% ==============================================================================
% SECTION : BIBLIOGRAPHY
% ==============================================================================
\bibliographystyle{plainnat}
\bibliography{bibliography}



% ==============================================================================
% SECTION : APPENDIX
% ==============================================================================
\newpage
\appendix
\section{Appendix: Appendix section}\label{sec:appendix}


\end{document}